# pip install pandas scikit-learn requests numpy matplotlib seaborn
import requests
import pandas as pd
import numpy as np
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
import itertools
import time
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib import font_manager, rc

# --------------------------------------------------------------------------
# âœ… ì„¤ì • ë³€ìˆ˜
# --------------------------------------------------------------------------
# â—ï¸â—ï¸ ë³¸ì¸ì˜ ì¹´ì¹´ì˜¤ REST API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”.
KAKAO_API_KEY = "6e5ff5f0fc34ba8dce84b422a33066bc"

# 1. í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„ ëŒ€ìƒì´ ë  ëŒ€í•œë¯¼êµ­ ëŒ€í‘œ ê´€ê´‘ ë„ì‹œ í›„ë³´êµ°
CANDIDATE_CITIES = [
    "ì„œìš¸ ì¢…ë¡œêµ¬", "ì„œìš¸ ì¤‘êµ¬", "ì„œìš¸ ê°•ë‚¨êµ¬", "ë¶€ì‚° í•´ìš´ëŒ€êµ¬", 
    "ë¶€ì‚° ì¤‘êµ¬", "ì œì£¼ì‹œ", "ì„œê·€í¬ì‹œ", "ê²½ì£¼ì‹œ", "ì „ì£¼ì‹œ ì™„ì‚°êµ¬", "ê°•ë¦‰ì‹œ"
]

# 2. ê° ë„ì‹œì˜ ë©´ì (kmÂ²) - (ì‚¬ì „ ì¡°ì‚¬ëœ ê³ ì •ê°’)
CITY_AREAS = {
    "ì„œìš¸ ì¢…ë¡œêµ¬": 23.91, "ì„œìš¸ ì¤‘êµ¬": 9.96, "ì„œìš¸ ê°•ë‚¨êµ¬": 39.5, "ë¶€ì‚° í•´ìš´ëŒ€êµ¬": 51.47,
    "ë¶€ì‚° ì¤‘êµ¬": 2.8, "ì œì£¼ì‹œ": 978.7, "ì„œê·€í¬ì‹œ": 870.8, "ê²½ì£¼ì‹œ": 1357.0,
    "ì „ì£¼ì‹œ ì™„ì‚°êµ¬": 95.2, "ê°•ë¦‰ì‹œ": 1040.0
}

# 3. ê´€ê´‘ íŠ¹ì„± ë¶„ì„ì„ ìœ„í•œ POI ì¹´í…Œê³ ë¦¬ (ì¹´ì¹´ì˜¤ë§µ API ê¸°ì¤€)
CATEGORY_CODES = {
    'attraction': 'AT4', # ê´€ê´‘ëª…ì†Œ
    'culture': 'CT1',    # ë¬¸í™”ì‹œì„¤
    'shopping': 'MT1',   # ì‡¼í•‘ (ëŒ€í˜•ë§ˆíŠ¸/ë°±í™”ì )
    'food': 'FD6',       # ìŒì‹ì 
}

# --------------------------------------------------------------------------
# í—¬í¼ í•¨ìˆ˜
# --------------------------------------------------------------------------
def get_total_poi_count(city, category_code):
    """íŠ¹ì • ë„ì‹œì™€ ì¹´í…Œê³ ë¦¬ì— ëŒ€í•œ ì „ì²´ POI ê°œìˆ˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    url = "https://dapi.kakao.com/v2/local/search/keyword.json"
    headers = {"Authorization": f"KakaoAK {KAKAO_API_KEY}"}
    params = {'query': city, 'category_group_code': category_code, 'size': 1}
    try:
        response = requests.get(url, headers=headers, params=params)
        if response.status_code == 200:
            return response.json()['meta']['total_count']
    except Exception:
        return 0
    return 0

def get_poi_coords(city, limit=20):
    """ë¶„ì‚°ë„ ê³„ì‚°ì„ ìœ„í•´ ë„ì‹œ ë‚´ ëŒ€í‘œ POIë“¤ì˜ ì¢Œí‘œë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
    url = "https://dapi.kakao.com/v2/local/search/keyword.json"
    headers = {"Authorization": f"KakaoAK {KAKAO_API_KEY}"}
    params = {'query': f"{city} ê´€ê´‘", 'size': limit}
    coords = []
    try:
        response = requests.get(url, headers=headers, params=params)
        if response.status_code == 200:
            for doc in response.json()['documents']:
                coords.append((float(doc['y']), float(doc['x']))) # (lat, lon)
    except Exception:
        return []
    return coords

def calculate_dispersion(coords):
    """ì¢Œí‘œ ëª©ë¡ ê°„ì˜ í‰ê·  ê±°ë¦¬ë¥¼ ê³„ì‚°í•˜ì—¬ ë¶„ì‚°ë„ë¥¼ ì¸¡ì •í•©ë‹ˆë‹¤."""
    if len(coords) < 2: return 0
    total_dist = 0
    count = 0
    for p1, p2 in itertools.combinations(coords, 2):
        # Haversine formula to calculate distance between two lat/lon points
        R = 6371  # Radius of Earth in km
        lat1, lon1 = np.radians(p1)
        lat2, lon2 = np.radians(p2)
        dlon = lon2 - lon1
        dlat = lat2 - lat1
        a = np.sin(dlat / 2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2)**2
        c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))
        distance = R * c
        total_dist += distance
        count += 1
    return total_dist / count if count > 0 else 0

# --------------------------------------------------------------------------
# ë©”ì¸ ì‹¤í–‰ ë¡œì§
# --------------------------------------------------------------------------
def run_city_clustering():
    print("--- [PRE-STEP] ê´€ê´‘ íŠ¹ì„± í´ëŸ¬ìŠ¤í„°ë§ì„ í†µí•œ ëŒ€í‘œ ì§€ì—­ ì„ ë°œ ì‹œì‘ ---")
    
    # 1. ê° í›„ë³´ ë„ì‹œë³„ 3ê°€ì§€ ì •ëŸ‰ ì§€í‘œ ê³„ì‚°
    city_features = []
    for city in CANDIDATE_CITIES:
        print(f"\n[{city}] ê´€ê´‘ íŠ¹ì„± ë¶„ì„ ì¤‘...")
        time.sleep(0.5)
        
        # ì§€í‘œ 1: ê´€ê´‘ìì› ë°€ì§‘ë„ (ê´€ê´‘ëª…ì†Œ+ë¬¸í™”ì‹œì„¤ ê°œìˆ˜ / ë©´ì )
        attraction_count = get_total_poi_count(city, CATEGORY_CODES['attraction'])
        culture_count = get_total_poi_count(city, CATEGORY_CODES['culture'])
        density = (attraction_count + culture_count) / CITY_AREAS[city]
        print(f"  - ë°€ì§‘ë„: {density:.2f} ê°œ/kmÂ²")

        # ì§€í‘œ 2: ê´€ê´‘ìì› ë‹¤ì–‘ì„± (ì—”íŠ¸ë¡œí”¼ ì§€ìˆ˜)
        category_counts = [get_total_poi_count(city, code) for code in CATEGORY_CODES.values()]
        total_pois = sum(category_counts)
        proportions = [count / total_pois for count in category_counts if count > 0]
        diversity = -sum(p * np.log2(p) for p in proportions if p > 0)
        print(f"  - ë‹¤ì–‘ì„±: {diversity:.2f}")

        # ì§€í‘œ 3: ê´€ê´‘ìì› ë¶„ì‚°ë„ (POI ê°„ í‰ê·  ê±°ë¦¬)
        coords = get_poi_coords(city)
        dispersion = calculate_dispersion(coords)
        print(f"  - ë¶„ì‚°ë„: {dispersion:.2f} km")
        
        city_features.append({
            'city': city,
            'density': density,
            'diversity': diversity,
            'dispersion': dispersion
        })

    df = pd.DataFrame(city_features)
    
    # 2. K-means í´ëŸ¬ìŠ¤í„°ë§ ì‹¤í–‰
    features = df[['density', 'diversity', 'dispersion']]
    scaler = StandardScaler()
    scaled_features = scaler.fit_transform(features)
    
    kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)
    df['cluster'] = kmeans.fit_predict(scaled_features)
    
    # 3. ê° í´ëŸ¬ìŠ¤í„°ì˜ ëŒ€í‘œ ì§€ì—­ ì„ ë°œ
    centroids = kmeans.cluster_centers_
    representatives = []
    for i in range(3):
        cluster_data = scaled_features[df['cluster'] == i]
        centroid = centroids[i]
        distances = np.linalg.norm(cluster_data - centroid, axis=1)
        closest_point_index = np.argmin(distances)
        representative_city_index = df[df['cluster'] == i].index[closest_point_index]
        representatives.append(df.loc[representative_city_index]['city'])

    # 4. ìµœì¢… ê²°ê³¼ ì¶œë ¥ ë° ì‹œê°í™”
    # í•œê¸€ í°íŠ¸ ì„¤ì •
    try:
        path = "c:/Windows/Fonts/malgun.ttf"
        font_name = font_manager.FontProperties(fname=path).get_name()
        rc('font', family=font_name)
    except:
        print("í•œê¸€ í°íŠ¸(Malgun Gothic)ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ì˜ë¬¸ìœ¼ë¡œ í‘œì‹œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    plt.figure(figsize=(12, 8))
    sns.scatterplot(data=df, x='dispersion', y='density', hue='cluster', s=200, palette='viridis', style='city', markers={city: 'o' for city in df['city']})
    plt.title('ê´€ê´‘ ë„ì‹œ í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ (ë°€ì§‘ë„ vs ë¶„ì‚°ë„)', fontsize=16)
    plt.xlabel('ê´€ê´‘ìì› ë¶„ì‚°ë„ (POI ê°„ í‰ê·  ê±°ë¦¬, km)', fontsize=12)
    plt.ylabel('ê´€ê´‘ìì› ë°€ì§‘ë„ (POI ê°œìˆ˜ / ë©´ì )', fontsize=12)
    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)
    plt.grid(True)
    plt.tight_layout()
    plt.show()

    print("\n" + "="*80)
    print("ğŸ‰ [PRE-STEP ì™„ë£Œ] ë°ì´í„° ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§ì„ í†µí•´ ì„ ë°œëœ ê° ìœ í˜•ë³„ ëŒ€í‘œ ì§€ì—­ ğŸ‰")
    print("="*80)
    
    for i, rep in enumerate(representatives):
        cluster_cities = df[df['cluster'] == i]['city'].tolist()
        print(f"\n[ìœ í˜• {i+1}] ëŒ€í‘œ ì§€ì—­: â­ {rep} â­")
        print(f"  - ì´ ìœ í˜•ì— ì†í•œ ë‹¤ë¥¸ ë„ì‹œë“¤: {', '.join(cluster_cities)}")

    print("\n\n[ìµœì¢… ê²°ë¡ ]")
    print("ìœ„ í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ëŠ”, ìš°ë¦¬ê°€ ë³¸ ë¶„ì„ì—ì„œ 'ì„œìš¸ ì¢…ë¡œêµ¬', 'ë¶€ì‚° í•´ìš´ëŒ€êµ¬', 'ì„œìš¸ ì¤‘êµ¬'ì™€ ê°™ì€ ì§€ì—­ë“¤ì„")
    print("ë¹„êµ ëŒ€ìƒìœ¼ë¡œ ì„ ì •í•œ ê²ƒì´ ì„ì˜ì˜ ì„ íƒì´ ì•„ë‹ˆë¼, ëŒ€í•œë¯¼êµ­ ê´€ê´‘ì§€ì˜ ë‹¤ì–‘í•œ ìœ í˜•ì„ ëŒ€í‘œí•˜ëŠ”")
    print("ê°ê´€ì ì´ê³  í•©ë¦¬ì ì¸ ì„ íƒì´ì—ˆìŒì„ ë°ì´í„°ë¡œ ì¦ëª…í•©ë‹ˆë‹¤.")

if __name__ == "__main__":
    run_city_clustering()