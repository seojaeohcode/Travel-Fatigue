import pandas as pd
import requests
import itertools
from sklearn.preprocessing import MinMaxScaler
import numpy as np
import os

# --------------------------------------------------------------------------
# ìŠ¤í¬ë¦½íŠ¸ì˜ ì‹¤ì œ ìœ„ì¹˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ìƒëŒ€ ê²½ë¡œ ì„¤ì •
# --------------------------------------------------------------------------
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
DATASET_DIR = os.path.join(BASE_DIR, "dataset")

# --------------------------------------------------------------------------
# âœ… 1. ì‚¬ìš©ì ì…ë ¥ (ì´ ë¶€ë¶„ë§Œ ìˆ˜ì •í•˜ì—¬ ì‚¬ìš©)
# --------------------------------------------------------------------------
USER_START_LOCATION = "í™ëŒ€ì…êµ¬ì—­"
USER_DESIRED_POIS = ["êµ­ë¦½ì¤‘ì•™ë°•ë¬¼ê´€", "Nì„œìš¸íƒ€ì›Œ", "ë”í˜„ëŒ€ ì„œìš¸"]

# â—ï¸â—ï¸ ë³¸ì¸ì˜ ì¹´ì¹´ì˜¤ REST API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”.
KAKAO_API_KEY = "6e5ff5f0fc34ba8dce84b422a33066bc"

# --------------------------------------------------------------------------
# âœ… 2. ì‚¬ì „ ì—°êµ¬ ë° ë°ì´í„° íŒŒì¼ ì„¤ì • (dataset í´ë” ì‚¬ìš©)
# --------------------------------------------------------------------------
RESEARCH_DATA_FILE = os.path.join(DATASET_DIR, "research_base_data.csv")
SPENDING_DATA_FILES = {
    "ì„œìš¸ ì¢…ë¡œêµ¬": os.path.join(DATASET_DIR, "spending_jongno.csv"),
    "ë¶€ì‚° í•´ìš´ëŒ€êµ¬": os.path.join(DATASET_DIR, "spending_haeundae.csv"),
    "ì„œìš¸ ì¤‘êµ¬": os.path.join(DATASET_DIR, "spending_junggu.csv")
}

# --------------------------------------------------------------------------
# í—¬í¼ í•¨ìˆ˜ (1ë‹¨ê³„ ì½”ë“œì™€ ë™ì¼)
# --------------------------------------------------------------------------
def get_coords_for_location(address):
    # ... (1ë‹¨ê³„ì™€ ë™ì¼í•œ ì¢Œí‘œ ë³€í™˜ í•¨ìˆ˜)
    url = "https://dapi.kakao.com/v2/local/search/keyword.json"
    headers = {"Authorization": f"KakaoAK {KAKAO_API_KEY}"}
    params = {'query': address, 'size': 1}
    try:
        response = requests.get(url, headers=headers, params=params)
        if response.status_code == 200 and response.json()['documents']:
            doc = response.json()['documents'][0]
            return {"name": doc['place_name'], "lon": float(doc['x']), "lat": float(doc['y'])}
    except Exception:
        return None
    return None

def get_public_transit_metrics(start_coords, end_coords):
    # ... (1ë‹¨ê³„ì™€ ë™ì¼í•œ ëŒ€ì¤‘êµí†µ ì§€í‘œ ì¶”ì¶œ í•¨ìˆ˜)
    url = "https://apis-navi.kakaomobility.com/v1/directions"
    headers = {"Authorization": f"KakaoAK {KAKAO_API_KEY}"}
    params = {"origin": f"{start_coords['lon']},{start_coords['lat']}", "destination": f"{end_coords['lon']},{end_coords['lat']}"}
    try:
        response = requests.get(url, headers=headers, params=params)
        if response.status_code != 200: return None
        data = response.json()
        if not data.get('routes') or data['routes'][0].get('result_code') != 0: return None
        route = data['routes'][0]
        summary = route['summary']
        total_walk_distance = 0
        transit_count = 0
        for section in route['sections']:
            if section.get('guides'):
                for guide in section['guides']:
                    if guide.get('name') == 'ë„ë³´':
                        total_walk_distance += guide.get('distance', 0)
            transit_count += sum(1 for guide in section.get('guides', []) if guide.get('type') in [1, 2])
        return {"distance": summary.get('distance', 0), "duration": summary.get('duration', 0), "walk_distance": total_walk_distance, "transfers": max(0, transit_count - 1), "fare": summary.get('fare', {}).get('total', 0)}
    except Exception:
        return None

# --------------------------------------------------------------------------
# [í•µì‹¬] ë°ì´í„° ê¸°ë°˜ ê°€ì¤‘ì¹˜ ë„ì¶œ í•¨ìˆ˜
# --------------------------------------------------------------------------
def get_data_driven_weights():
    """ì‚¬ì „ ì—°êµ¬ ë°ì´í„°ì™€ ì†Œë¹„ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ TPFI ê°€ì¤‘ì¹˜ë¥¼ ë„ì¶œí•©ë‹ˆë‹¤."""
    print("--- [ì‚¬ì „ ë¶„ì„] ë°ì´í„° ê¸°ë°˜ TPFI ê°€ì¤‘ì¹˜ ë„ì¶œ ì‹œì‘ ---")
    try:
        df_research = pd.read_csv(RESEARCH_DATA_FILE)
    except FileNotFoundError:
        print(f"âŒ ì—ëŸ¬: ì‚¬ì „ ì—°êµ¬ ë°ì´í„°('{RESEARCH_DATA_FILE}')ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        return None

    # 1. ì§€ì—­ë³„ í‰ê·  ì´ë™ ì§€í‘œ ê³„ì‚°
    travel_metrics = df_research.groupby('region').agg(
        avg_distance_m=('distance', 'mean'),
        avg_transfers=('transfers', 'mean'),
        avg_walk_ratio=('walk_ratio', 'mean')
    ).reset_index()

    # 2. 'ì„ íƒì  í™œë ¥ ì†Œë¹„' ì§€í‘œ ê³„ì‚°
    consumption_results = []
    for region, filename in SPENDING_DATA_FILES.items():
        try:
            df_spending = pd.read_csv(filename)
            vitality_ratio = df_spending[df_spending['ëŒ€ë¶„ë¥˜'].isin(['ì‡¼í•‘ì—…', 'ì—¬ê°€ì„œë¹„ìŠ¤ì—…'])]['ëŒ€ë¶„ë¥˜ ì§€ì¶œì•¡ ë¹„ìœ¨'].unique().sum()
            consumption_results.append({'region': region, 'vitality_consumption_ratio': vitality_ratio})
        except FileNotFoundError: return None
    df_consumption = pd.DataFrame(consumption_results)

    # 3. ë°ì´í„° ë³‘í•© ë° ìƒê´€ê´€ê³„ ë¶„ì„
    df_merged = pd.merge(travel_metrics, df_consumption, on='region')
    correlations = df_merged.corr(numeric_only=True)['vitality_consumption_ratio']
    fatigue_factors = correlations[correlations < 0].drop('vitality_consumption_ratio', errors='ignore')

    if fatigue_factors.empty: return None

    # 4. ìƒê´€ê³„ìˆ˜ë¥¼ ê°€ì¤‘ì¹˜ë¡œ ë³€í™˜
    weights_raw = abs(fatigue_factors)
    data_driven_weights = (weights_raw / weights_raw.sum()).to_dict()
    
    print("âœ… ë°ì´í„° ê¸°ë°˜ ê°€ì¤‘ì¹˜ ë„ì¶œ ì™„ë£Œ.")
    return {
        'distance': data_driven_weights.get('avg_distance_m', 0),
        'transfers': data_driven_weights.get('avg_transfers', 0),
        'walk_ratio': data_driven_weights.get('avg_walk_ratio', 0)
    }

# --------------------------------------------------------------------------
# ë©”ì¸ ì†”ë£¨ì…˜ ì‹¤í–‰ ë¡œì§
# --------------------------------------------------------------------------
def calculate_user_trip_fatigue(tpfi_weights):
    """ì‚¬ìš©ì ì…ë ¥ì„ ë°›ì•„ ëª¨ë“  ê²½ë¡œ í›„ë³´ì˜ í”¼ë¡œë„ë¥¼ ê³„ì‚°í•˜ê³  ë¹„êµ ì œì‹œí•©ë‹ˆë‹¤."""
    print("\n--- [ì†”ë£¨ì…˜ ì‹¤í–‰] ì‚¬ìš©ì ì§€ì • ê²½ë¡œ ìµœì†Œ í”¼ë¡œë„ ì‚°ì¶œ ì‹œì‘ ---")
    
    # 1. ì‚¬ìš©ì ì…ë ¥ ì¢Œí‘œ ë³€í™˜
    start_coords = get_coords_for_location(USER_START_LOCATION)
    poi_coords = [get_coords_for_location(poi) for poi in USER_DESIRED_POIS]
    if not start_coords or None in poi_coords:
        print("âŒ ì…ë ¥í•œ ì¥ì†Œì˜ ì¢Œí‘œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì¥ì†Œëª…ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return

    # 2. ê²½ë¡œ í›„ë³´ ìƒì„± ë° ì´ë™ ì§€í‘œ ê³„ì‚°
    all_routes = []
    for permutation in itertools.permutations(poi_coords):
        complete_loop = [start_coords] + list(permutation) + [start_coords]
        total_metrics = {"distance": 0, "duration": 0, "walk_distance": 0, "transfers": 0}
        is_valid = True
        for i in range(len(complete_loop) - 1):
            metrics = get_public_transit_metrics(complete_loop[i], complete_loop[i+1])
            if metrics is None: is_valid = False; break
            for key in total_metrics: total_metrics[key] += metrics[key]
        
        if is_valid:
            total_metrics['name'] = " -> ".join([p['name'] for p in complete_loop])
            all_routes.append(total_metrics)
    
    if not all_routes:
        print("âŒ ìœ íš¨í•œ ëŒ€ì¤‘êµí†µ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    # 3. TPFI ì ìˆ˜ ê³„ì‚°
    df = pd.DataFrame(all_routes)
    df['walk_ratio'] = (df['walk_distance'] / df['distance']).fillna(0)
    
    scaler = MinMaxScaler()
    features = ['distance', 'transfers', 'walk_ratio']
    df_scaled = df.copy()
    df_scaled[features] = scaler.fit_transform(df[features])

    df['tpfi_score'] = (df_scaled['distance'] * tpfi_weights['distance'] +
                        df_scaled['transfers'] * tpfi_weights['transfers'] +
                        df_scaled['walk_ratio'] * tpfi_weights['walk_ratio']) * 100
    
    # 4. ìµœì¢… ê²°ê³¼ ì¶œë ¥
    final_report = df[['name', 'tpfi_score', 'duration', 'walk_distance']].sort_values('tpfi_score')
    
    print("\n" + "="*70)
    print(f"ğŸ—ºï¸  '{USER_START_LOCATION}'ì—ì„œ ì¶œë°œí•˜ëŠ” ë‹¹ì‹ ì˜ ì—¬í–‰ ê³„íš í”¼ë¡œë„ ë¶„ì„ ê²°ê³¼")
    print("="*70)
    
    for i, row in final_report.iterrows():
        print(f"ğŸ“ ê²½ë¡œ ìˆœì„œ: {row['name']}")
        print(f"   - ğŸ”¥ ìµœì†Œ í”¼ë¡œë„(TPFI) ì ìˆ˜: {row['tpfi_score']:.1f} ì  (ë‚®ì„ìˆ˜ë¡ ëœ í”¼ê³¤)")
        print(f"   - ğŸ•’ ì˜ˆìƒ ì´ ì´ë™ì‹œê°„: ì•½ {row['duration']/3600:.1f} ì‹œê°„")
        print(f"   - ğŸš¶ ì˜ˆìƒ ì´ ë„ë³´ê±°ë¦¬: ì•½ {row['walk_distance']/1000:.1f} km\n")

if __name__ == "__main__":
    # 1. ë¨¼ì € ë°ì´í„° ê¸°ë°˜ ê°€ì¤‘ì¹˜ë¥¼ ì–»ê³ ,
    weights = get_data_driven_weights()
    
    # 2. ê·¸ ê°€ì¤‘ì¹˜ë¥¼ ì´ìš©í•´ ì‚¬ìš©ì ê²½ë¡œì˜ í”¼ë¡œë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    if weights:
        calculate_user_trip_fatigue(weights)
    else:
        print("\nğŸš¨ ë°ì´í„° ê¸°ë°˜ ê°€ì¤‘ì¹˜ë¥¼ ë„ì¶œí•  ìˆ˜ ì—†ì–´ í”¼ë¡œë„ ê³„ì‚°ì„ ì§„í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")