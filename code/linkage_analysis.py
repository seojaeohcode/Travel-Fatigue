import pandas as pd
from sklearn.preprocessing import MinMaxScaler
import numpy as np
import os

# --------------------------------------------------------------------------
# âœ… ì„¤ì • ë³€ìˆ˜
# --------------------------------------------------------------------------

# ìŠ¤í¬ë¦½íŠ¸ì˜ ì‹¤ì œ ìœ„ì¹˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ìƒëŒ€ ê²½ë¡œ ì„¤ì •
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
DATASET_DIR = os.path.join(BASE_DIR, "dataset")

# ë„ë³´ ë²„ê·¸ë¥¼ ìˆ˜ì •í•œ, ê°€ì¥ ìµœì‹  ì´ë™ ê²½ë¡œ ë°ì´í„° íŒŒì¼ëª…
ITINERARY_DATA_FILE = os.path.join(DATASET_DIR, "corrected_itinerary_metrics.csv")

SPENDING_DATA_FILES = {
    "ì„œìš¸ ì¢…ë¡œêµ¬": os.path.join(DATASET_DIR, "spending_jongno.csv"),
    "ë¶€ì‚° í•´ìš´ëŒ€êµ¬": os.path.join(DATASET_DIR, "spending_haeundae.csv"),
    "ì„œìš¸ ì¤‘êµ¬": os.path.join(DATASET_DIR, "spending_junggu.csv")
}

# --------------------------------------------------------------------------
# ë¶„ì„ í•¨ìˆ˜
# --------------------------------------------------------------------------
def run_data_driven_tpfi_analysis():
    """
    ì´ë™ íŒ¨í„´ê³¼ ì†Œë¹„ ë°ì´í„°ì˜ ìƒê´€ê´€ê³„ë¥¼ ë¶„ì„í•˜ì—¬
    ë°ì´í„° ê¸°ë°˜ì˜ TPFI ê°€ì¤‘ì¹˜ë¥¼ ë„ì¶œí•˜ê³ , ìµœì¢… í”¼ë¡œë„ ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    """
    print("[1ë‹¨ê³„] ì´ë™ íŒ¨í„´ ë°ì´í„° ë¡œë“œ ë° ìš”ì•½...")
    try:
        df_itinerary = pd.read_csv(ITINERARY_DATA_FILE)
    except FileNotFoundError:
        print(f"âŒ ì—ëŸ¬: '{ITINERARY_DATA_FILE}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("   - ì´ì „ ë‹¨ê³„ì˜ 'calculate_tpfi.py'ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì—¬ ë°ì´í„° íŒŒì¼ì„ ìƒì„±í•´ì£¼ì„¸ìš”.")
        return

    # ì§€ì—­ë³„ 4ëŒ€ ì´ë™ ì§€í‘œ í‰ê·  ê³„ì‚°
    travel_metrics = df_itinerary.groupby('region').agg(
        avg_distance_m=('total_distance_m', 'mean'),
        avg_duration_sec=('total_duration_sec', 'mean'),
        avg_transfers=('total_transfers', 'mean'),
        avg_walk_ratio=('walk_ratio', 'mean')
    ).reset_index()
    print("âœ… ì§€ì—­ë³„ í‰ê·  ì´ë™ ì§€í‘œ ê³„ì‚° ì™„ë£Œ.")

    # --- 2ë‹¨ê³„: ì†Œë¹„ íŒ¨í„´ ë°ì´í„° ë¶„ì„ ë° 'ì„ íƒì  í™œë ¥ ì†Œë¹„' ì§€í‘œ ê³„ì‚° ---
    print("\n[2ë‹¨ê³„] ì†Œë¹„ íŒ¨í„´ ë¶„ì„ ë° 'ì„ íƒì  í™œë ¥ ì†Œë¹„' ì§€í‘œ ìƒì„±...")
    consumption_results = []
    for region, filename in SPENDING_DATA_FILES.items():
        try:
            df_spending = pd.read_csv(filename)
            # 'ì‡¼í•‘ì—…'ê³¼ 'ì—¬ê°€ì„œë¹„ìŠ¤ì—…'ì˜ ì§€ì¶œì•¡ ë¹„ìœ¨ì„ í•©ì‚°
            vitality_ratio = df_spending[df_spending['ëŒ€ë¶„ë¥˜'].isin(['ì‡¼í•‘ì—…', 'ì—¬ê°€ì„œë¹„ìŠ¤ì—…'])]['ëŒ€ë¶„ë¥˜ ì§€ì¶œì•¡ ë¹„ìœ¨'].unique().sum()
            consumption_results.append({'region': region, 'vitality_consumption_ratio': vitality_ratio})
        except FileNotFoundError:
            print(f"âš ï¸  ê²½ê³ : '{filename}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            continue
    
    df_consumption = pd.DataFrame(consumption_results)
    print("âœ… ì§€ì—­ë³„ 'ì„ íƒì  í™œë ¥ ì†Œë¹„' ë¹„ìœ¨ ê³„ì‚° ì™„ë£Œ.")

    # --- 3ë‹¨ê³„: ë°ì´í„° ë³‘í•© ë° ìƒê´€ê´€ê³„ ë¶„ì„ ---
    print("\n[3ë‹¨ê³„] ìƒê´€ê´€ê³„ ë¶„ì„ì„ í†µí•œ ê°€ì¤‘ì¹˜ ì˜í–¥ë ¥ íŒŒì•…...")
    df_merged = pd.merge(travel_metrics, df_consumption, on='region')
    
    # ìƒê´€ê´€ê³„ í–‰ë ¬ ê³„ì‚°
    correlation_matrix = df_merged.corr(numeric_only=True)
    # 'ì„ íƒì  í™œë ¥ ì†Œë¹„ ë¹„ìœ¨'ê³¼ì˜ ìƒê´€ê´€ê³„ë§Œ ì¶”ì¶œ
    correlations = correlation_matrix['vitality_consumption_ratio'].drop('vitality_consumption_ratio')
    print("âœ… ì´ë™ ìš”ì¸ê³¼ í™œë ¥ ì†Œë¹„ ê°„ì˜ ìƒê´€ê´€ê³„ ê³„ì‚° ì™„ë£Œ:")
    print(correlations)

    # --- 4ë‹¨ê³„: ìƒê´€ê´€ê³„ë¥¼ ì´ìš©í•œ ë°ì´í„° ê¸°ë°˜ TPFI ê°€ì¤‘ì¹˜ ë„ì¶œ ---
    print("\n[4ë‹¨ê³„] ë°ì´í„° ê¸°ë°˜ TPFI ê°€ì¤‘ì¹˜ ë„ì¶œ...")
    # ìŒì˜ ìƒê´€ê´€ê³„ë¥¼ ê°€ì§„ ìš”ì¸ë§Œ í•„í„°ë§ (í”¼ë¡œ ìš”ì¸ì´ë¯€ë¡œ ìŒì˜ ê´€ê³„ê°€ ì •ìƒ)
    fatigue_factors = correlations[correlations < 0]
    
    if fatigue_factors.empty:
        print("âŒ ë¶„ì„ ì¤‘ë‹¨: í”¼ë¡œ ìš”ì¸ìœ¼ë¡œ ì¶”ì •ë˜ëŠ” ìŒì˜ ìƒê´€ê´€ê³„ë¥¼ ê°€ì§„ ì§€í‘œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
        
    # ì ˆëŒ“ê°’ì˜ í•©ìœ¼ë¡œ ì •ê·œí™”í•˜ì—¬ ê°€ì¤‘ì¹˜ ê³„ì‚°
    weights_raw = abs(fatigue_factors)
    data_driven_weights = (weights_raw / weights_raw.sum()).to_dict()
    
    print("âœ… ìµœì¢… TPFI ê°€ì¤‘ì¹˜ ê³„ì‚° ì™„ë£Œ:")
    for factor, weight in data_driven_weights.items():
        print(f"   - w_{factor.replace('avg_', '')}: {weight:.4f}")

    # --- 5ë‹¨ê³„: ìµœì¢… TPFI ì ìˆ˜ ê³„ì‚° ë° ê²°ê³¼ ì¶œë ¥ ---
    print("\n[5ë‹¨ê³„] ìµœì¢… TPFI ì ìˆ˜ ê³„ì‚° ë° ì§€ì—­ë³„ í”¼ë¡œë„ ë¶„ì„...")
    features_for_tpfi = {
        'total_distance_m': data_driven_weights.get('avg_distance_m', 0),
        'total_duration_sec': data_driven_weights.get('avg_duration_sec', 0),
        'total_transfers': data_driven_weights.get('avg_transfers', 0),
        'walk_ratio': data_driven_weights.get('avg_walk_ratio', 0)
    }
    
    scaler = MinMaxScaler()
    df_itinerary_scaled = df_itinerary.copy()
    feature_keys = list(features_for_tpfi.keys())
    df_itinerary_scaled[feature_keys] = scaler.fit_transform(df_itinerary[feature_keys])

    df_itinerary['tpfi_score'] = sum(df_itinerary_scaled[key] * weight for key, weight in features_for_tpfi.items())

    print("\n" + "="*60)
    print("ğŸ‰ ìµœì¢… ë¶„ì„ ê²°ê³¼: ë°ì´í„° ê¸°ë°˜ TPFI ì§€ì—­ë³„ í‰ê·  ì ìˆ˜ ğŸ‰")
    print("="*60)
    
    final_report = df_itinerary.groupby('region')['tpfi_score'].mean().sort_values(ascending=False).reset_index()
    final_report.columns = ['ì§€ì—­', 'ë°ì´í„° ê¸°ë°˜ í‰ê·  TPFI']
    print(final_report.to_markdown(index=False))

    print("\n[ìµœì¢… ê²°ë¡ ]")
    print("ìƒê´€ê´€ê³„ ë¶„ì„ì„ í†µí•´ 'í™œë ¥ ì†Œë¹„'ë¥¼ ê°€ì¥ ë§ì´ ê°ì†Œì‹œí‚¤ëŠ” ì´ë™ ìš”ì¸ì„ ì°¾ì•„ë‚´ê³ ,")
    print("ê·¸ ì˜í–¥ë ¥ì„ ë°”íƒ•ìœ¼ë¡œ ê°ê´€ì ì¸ TPFI ê°€ì¤‘ì¹˜ë¥¼ ë„ì¶œí–ˆìŠµë‹ˆë‹¤.")
    print("ì´ ì ìˆ˜ëŠ” ê° ì§€ì—­ì˜ ê°ê´€ì ì¸ 'ì´ë™ ë¶€í•˜'ë¥¼ ê°€ì¥ ë…¼ë¦¬ì ìœ¼ë¡œ ë‚˜íƒ€ë‚´ëŠ” ì§€í‘œì…ë‹ˆë‹¤.")

if __name__ == "__main__":
    run_data_driven_tpfi_analysis()
